"""
Generador de Cuentos con LangChain ğŸ“š
Este script usa LangChain de manera asÃ­ncrona para generar cuentos interactivos,
manteniendo la coherencia de la historia usando un historial de mensajes.
"""

import asyncio
import config
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# 1. ConfiguraciÃ³n del modelo de lenguaje
def crear_modelo():
    """Crea y configura el modelo de ChatOpenAI."""
    return ChatOpenAI(
        api_key=config.OPENAI_API_KEY,
        model="gpt-4o-2024-08-06",
        temperature=0.8  # Mayor temperatura = mÃ¡s creatividad
    )

# 2. ConfiguraciÃ³n del prompt
def crear_prompt():
    """Crea el template del prompt con el sistema y el historial."""
    return ChatPromptTemplate.from_messages([
        # Instrucciones para el modelo
        ("system", """
        Eres un narrador muy creativo que:
        - Escribe historias con mucho diÃ¡logo
        - Describe brevemente a los personajes
        - Hace que animales y objetos puedan hablar
        - Mantiene coherencia con las partes anteriores de la historia
        """),
        # Placeholder para el historial de la conversaciÃ³n
        MessagesPlaceholder(variable_name="history"),
        # Input del usuario
        ("human", "{input}")
    ])

# 3. ConfiguraciÃ³n de la cadena con historial
def crear_cadena_con_historial(modelo, prompt):
    """Crea una cadena que mantiene el historial de la conversaciÃ³n."""
    # Crear la cadena bÃ¡sica
    cadena = prompt | modelo
    
    # Crear el historial de mensajes
    historial = ChatMessageHistory()
    
    # Crear la cadena con historial
    return RunnableWithMessageHistory(
        cadena,
        lambda session_id: historial,
        input_messages_key="input",
        history_messages_key="history"
    ), historial

async def generar_cuento(cadena, trama: str = "", maximo_tokens: int = 500) -> str:
    """
    Genera una parte del cuento basado en la trama proporcionada.
    
    Args:
        cadena: La cadena de LangChain con historial
        trama: La trama para continuar la historia
        maximo_tokens: Longitud mÃ¡xima de la respuesta
    
    Returns:
        str: La siguiente parte del cuento
    """
    config = {
        "configurable": {
            "session_id": "story_session",
            "max_tokens": maximo_tokens
        }
    }
    
    # Usar ainvoke para llamada asÃ­ncrona
    response = await cadena.ainvoke(
        {"input": f"ContinÃºa o comienza el cuento basado en esta trama: {trama}"},
        config
    )
    return response.content

async def main():
    """FunciÃ³n principal asiÌncrona que ejecuta el generador de cuentos."""
    # Inicializar componentes
    modelo = crear_modelo()
    prompt = crear_prompt()
    cadena, historial = crear_cadena_con_historial(modelo, prompt)
    
    print("ğŸŒŸ Generador de Cuentos con IA ğŸ“š")
    print("Escribe las tramas de tu historia. El modelo irÃ¡ generando el cuento parte por parte.")
    print("Para terminar, escribe 'salir'")
    print("-" * 50)
    
    # Loop principal
    while True:
        trama = input("\nâœï¸  Como sigue tu trama: ")
        if trama.lower() == "salir":
            break
        
        # Esperar la respuesta asÃ­ncrona
        print("\nâŒ› Generando...", end="", flush=True)
        parte = await generar_cuento(cadena, trama)
        print("\r", end="")  # Limpiar el mensaje de "Generando..."
        print(f"\nğŸ“ Nueva parte del cuento:\n{parte}\n")
        print("-" * 50)

    # Mostrar historia completa
    print("\nğŸ“š Historia completa:\n")
    historia_completa = "\n".join([
        msg.content 
        for msg in historial.messages 
        if isinstance(msg, AIMessage)
    ])
    print(historia_completa)

if __name__ == "__main__":
    # Ejecutar el loop de eventos de asyncio
    asyncio.run(main())
